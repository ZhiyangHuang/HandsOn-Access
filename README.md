# HandsOn-Access

> ğŸ¯ **Hands-free Humanâ€“Computer Interaction System**
> A multimodal access system based on **Face Recognition + Head Movement + Voice Recognition**, designed for accessibility and hands-free control on Windows.

---

## âœ¨ Project Overview

**HandsOn-Access** integrates multiple AI technologies to enable users to control a computer without using hands:

* ğŸ‘¤ **Face recognition** for user identity verification
* ğŸ™‚ **Face & head movement detection** for mouse control
* ğŸ¤ **Speech recognition** (command-level & dictation)
* âŒ¨ï¸ **Voice-driven keyboard & clipboard control**

This project is especially suitable for:

* Accessibility / assistive technology
* Humanâ€“Computer Interaction (HCI) research
* AI + CV + Speech integration demos

---

## ğŸ§  Technologies Used

* **InsightFace** â€“ Face recognition
* **MediaPipe Face Landmarker** â€“ Face & head pose tracking
* **Whisper (OpenAI)** â€“ High-accuracy speech-to-text
* **Vosk** â€“ Lightweight command-based speech recognition
* **OpenCV** â€“ Real-time camera processing
* **PyTorch** â€“ Model inference backend
* **Tkinter** â€“ GUI interface (Windows)

---

## ğŸ“¦ Environment Requirements

* OS: **Windows 10 / 11**
* Python: **3.9 â€“ 3.10 (recommended)**
* GPU: Optional (CUDA supported but not required)

---

## ğŸ“š Python Dependencies

Install required packages:

```bash
pip install opencv-python numpy mediapipe insightface torch torchvision torchaudio
pip install sounddevice vosk whisper playsound pywin32
```

---

## ğŸ§  AI Model Download & Placement Guide (IMPORTANT)

Some AI models are **auto-downloaded to system cache**, while others **must be manually placed** in the `Model/` directory.

---

### ğŸ”¹ Whisper (OpenAI Speech Recognition)

#### ğŸ“Œ Where to find Whisper model download links?

All official Whisper model URLs are defined here:

ğŸ‘‰ [https://github.com/openai/whisper/blob/main/whisper/__init__.py](https://github.com/openai/whisper/blob/main/whisper/__init__.py)

Inside this file you will find:

```python
_MODELS = {
    "tiny": "...",
    "base": "...",
    "small": "...",
    "medium": "...",
    "large": "...",
}
```

Each entry corresponds to an official model download link.

---

#### ğŸ“‚ Whisper default installation location

Whisper models are automatically downloaded to:

```
C:\Users\<YourUsername>\.cache\whisper\
```

Example:

```
C:\Users\ZhiyangHuang\.cache\whisper\base.pt
```

âš ï¸ This is **normal behavior**. You do NOT need to move these files.

---

### ğŸ”¹ InsightFace (Face Recognition)

#### ğŸ“¥ Official model zoo

ğŸ‘‰ [https://github.com/deepinsight/insightface/tree/master/model_zoo](https://github.com/deepinsight/insightface/tree/master/model_zoo)

Recommended model:

```
buffalo_l
```

---

#### ğŸ“‚ InsightFace default installation location

By default, InsightFace downloads models to:

```
C:\Users\<YourUsername>\.insightface\models\
```

Example:

```
C:\Users\ZhiyangHuang\.insightface\models\buffalo_l\
```

You may also manually place the model inside the project:

```
Model/buffalo_l/
```

---

### ğŸ”¹ Vosk (Command-level Speech Recognition)

#### ğŸ“¥ Download Vosk models

ğŸ‘‰ [https://alphacephei.com/vosk/models](https://alphacephei.com/vosk/models)

Recommended English model:

```
vosk-model-small-en-us-0.15
```

---

#### ğŸ“‚ Required placement (IMPORTANT)

After downloading and extracting, **rename the folder to `Vosk`** and place it here:

```
Model/Vosk/
```

Directory structure example:

```
Model/
 â””â”€ Vosk/
    â”œâ”€ am
    â”œâ”€ conf
    â”œâ”€ graph
    â””â”€ ivector
```

---

### ğŸ”¹ MediaPipe Face Landmarker

#### ğŸ“¥ Download model

Official page:

ğŸ‘‰ [https://developers.google.com/mediapipe/solutions/vision/face_landmarker](https://developers.google.com/mediapipe/solutions/vision/face_landmarker)

Direct download (.task file):

ğŸ‘‰ [https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/latest/face_landmarker.task](https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/latest/face_landmarker.task)

---

#### ğŸ“‚ Required placement

```
Model/face_landmarker.task
```

---

## âœ… Model Placement Summary

| Model          | Download Method | Location                          |
| -------------- | --------------- | --------------------------------- |
| Whisper        | Auto / Manual   | `C:\Users\<User>\.cache\whisper\` |
| InsightFace    | Auto / Manual   | `C:\Users\<User>\.insightface\`   |
| Vosk           | Manual          | `Model/Vosk/`                     |
| MediaPipe Face | Manual          | `Model/face_landmarker.task`      |

---

## â–¶ï¸ How to Run

```bash
python main.py
```

> Make sure your **camera and microphone** are connected and accessible.

---

## ğŸ“‚ Recommended Project Structure

```
HandsOn-Access/
 â”œâ”€ Model/
 â”‚  â”œâ”€ Vosk/
 â”‚  â”œâ”€ buffalo_l/
 â”‚  â””â”€ face_landmarker.task
 â”œâ”€ Keyboard/
 â”œâ”€ main.py
 â”œâ”€ user_setting.json
 â””â”€ README.md
```

---

## âš ï¸ Notes

* This project is **Windows-only** (uses `pywin32`)
* Microphone permission is required
* First run may take time due to model downloads

---

## ğŸ¤ Credits

* OpenAI Whisper
* InsightFace
* MediaPipe
* Vosk Speech Recognition

---

## ğŸ“¬ Contact

If you have questions or want to contribute, feel free to open an issue on GitHub.

ğŸš€ Enjoy hands-free computing!
