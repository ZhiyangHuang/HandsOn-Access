# HandsOn-Access

> ğŸ¯ **Hands-free Humanâ€“Computer Interaction System**
> A multimodal access system based on **Face Recognition + Head Movement + Voice Recognition**, designed for accessibility and hands-free control on Windows.

---

## âœ¨ Project Overview

**HandsOn-Access** integrates multiple AI technologies to enable users to control a computer without using hands:

* ğŸ‘¤ **Face recognition** for user identity verification
* ğŸ™‚ **Face & head movement detection** for mouse control
* ğŸ¤ **Speech recognition** (command-level & dictation)
* âŒ¨ï¸ **Voice-driven keyboard & clipboard control**

This project is especially suitable for:

* Accessibility / assistive technology
* Humanâ€“Computer Interaction (HCI) research
* AI + CV + Speech integration demos

---

## ğŸ§  Technologies Used

* **InsightFace** â€“ Face recognition
* **MediaPipe Face Landmarker** â€“ Face & head pose tracking
* **Whisper (OpenAI)** â€“ High-accuracy speech-to-text
* **Vosk** â€“ Lightweight command-based speech recognition
* **OpenCV** â€“ Real-time camera processing
* **PyTorch** â€“ Model inference backend
* **Tkinter** â€“ GUI interface (Windows)

---

## ğŸ“¦ Environment Requirements

* OS: **Windows 10 / 11**
* Python: **3.9 â€“ 3.10 (recommended)**
* GPU: Optional (CUDA supported but not required)

---

## ğŸ“š Python Dependencies

Install required packages:

```bash
pip install opencv-python numpy mediapipe insightface torch torchvision torchaudio
pip install sounddevice vosk whisper playsound pywin32
```

---

## ğŸ§  AI Model Download & Placement Guide (IMPORTANT)

Some AI models are **auto-downloaded to system cache**, while others **must be manually placed** in the `Model/` directory.

---

### ğŸ”¹ Whisper (OpenAI Speech Recognition)

#### ğŸ“Œ Where to find Whisper model download links?

All official Whisper model URLs are defined here:

ğŸ‘‰ [https://github.com/openai/whisper/blob/main/whisper/__init__.py](https://github.com/openai/whisper/blob/main/whisper/__init__.py)

Inside this file you will find:

```python
_MODELS = {
    "tiny": "...",
    "base": "...",
    "small": "...",
    "medium": "...",
    "large": "...",
}
```

Each entry corresponds to an official model download link.

---

#### ğŸ“‚ Whisper default installation location

Whisper models are automatically downloaded to:

```
C:\Users\<YourUsername>\.cache\whisper\
```

âš ï¸ If you want to **download manually**, please move the **.pt** file to the corresponding folder.

---

### ğŸ”¹ InsightFace (Face Recognition)

#### ğŸ“¥ Official model zoo

ğŸ‘‰ [https://github.com/deepinsight/insightface/tree/master/model_zoo](https://github.com/deepinsight/insightface/tree/master/model_zoo)

Recommended model:

```
buffalo_l
```

---

#### ğŸ“‚ InsightFace default installation location

By default, InsightFace downloads models to:

```
C:\Users\<YourUsername>\.insightface\models\
```

---

### ğŸ”¹ Vosk (Command-level Speech Recognition)

#### ğŸ“¥ Download Vosk models

ğŸ‘‰ [https://alphacephei.com/vosk/models](https://alphacephei.com/vosk/models)

Recommended English model:

```
vosk-model-small-en-us-0.15
```

---

#### ğŸ“‚ Required placement (IMPORTANT)

After downloading and extracting, **rename the folder to `Vosk`** and place it here:

```
Model/Vosk/
```

Directory structure example:

```
Model/
 â””â”€ Vosk/
    â”œâ”€ am
    â”œâ”€ conf
    â”œâ”€ graph
    â””â”€ ivector
```

---

### ğŸ”¹ MediaPipe Face Landmarker

#### ğŸ“¥ Download model

Official page:

ğŸ‘‰ [https://developers.google.com/mediapipe/solutions/vision/face_landmarker](https://developers.google.com/mediapipe/solutions/vision/face_landmarker)

Direct download (.task file):

ğŸ‘‰ [https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/latest/face_landmarker.task](https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/latest/face_landmarker.task)

---

#### ğŸ“‚ Required placement

```
Model/face_landmarker.task
```

âš ï¸ If you want to **download manually**, please move the **buffalo_l/.onnx** file to the corresponding folder.

---

## âœ… Model Placement Summary

| Model          | Download Method | Location                          |
| -------------- | --------------- | --------------------------------- |
| Whisper        | Auto / Manual   | `C:\Users\<User>\.cache\whisper\` |
| InsightFace    | Auto / Manual   | `C:\Users\<User>\.insightface\`   |
| Vosk           | Manual          | `Model/Vosk/`                     |
| MediaPipe Face | Manual          | `Model/face_landmarker.task`      |

---

## â–¶ï¸ How to Run

```bash
python main.py
```

> Make sure your **camera and microphone** are connected and accessible.

---

## ğŸ“‚ Recommended Project Structure

```
HandsOn-Access/
 â”œâ”€ Model/
 â”‚  â”œâ”€ Vosk/
 â”‚  â”œâ”€ buffalo_l/
 â”‚  â””â”€ face_landmarker.task
 â”œâ”€ Keyboard/
 â”œâ”€ main.py
 â”œâ”€ user_setting.json
 â””â”€ README.md
```

---

## âš ï¸ Notes

* This project is **Windows-only** (uses `pywin32`)
* Microphone permission is required
* First run may take time due to model downloads

---

## ğŸ¤ Credits

* OpenAI Whisper
* InsightFace
* MediaPipe
* Vosk Speech Recognition

---

## ğŸ“¬ Contact Â· Feedback Â· Collaboration

HandsOn-Access is not a â€œfinishedâ€ project.  
It is an ongoing exploration of accessible humanâ€“computer interaction.

The core goal of this project is to support people who **cannot use a keyboard and mouse at the same time**, or who face barriers with traditional input devices, by providing **more natural and flexible ways to interact with computers**.  
At the same time, we recognize that **a single developerâ€™s perspective is always limited**.

### ğŸŒ± Directions We Hope to Explore Together

- ğŸ§â€â™‚ï¸ **More diverse gestures and orientations**  
  Different users have different physical abilities, motion ranges, and comfort zones.  
  Future work aims to support a wider variety of **head orientations, facial movements, and subtle posture changes**, instead of relying on a single â€œstandardâ€ gesture.

- ğŸ’» **Cross-platform accessibility**  
  The current implementation focuses on Windows, but the long-term goal is to gradually explore support for **other platforms such as Linux and macOS**.

- â™¿ **Real-world usage feedback**  
  Which gestures cause fatigue?  
  Which voice commands feel natural?  
  Which interactions look good in theory but fail in practice?  
  These insights can only come from real users and real scenarios.

- ğŸ”Œ **Extensible input and interaction methods**  
  Including, but not limited to:  
  eye tracking, external switches, hybrid inputs, and multi-modal interaction designs.

### ğŸ’¬ How You Can Participate

Whether you are:

- a user with accessibility needs  
- someone interested in accessibility and inclusive design  
- a developer curious about computer vision, speech recognition, or HCI  
- or simply someone with an idea that starts with â€œwhat ifâ€¦â€

You are very welcome to:

- open an Issue  
- share usage experiences  
- propose ideas or improvement plans  
- participate in discussions or contribute code

Even a small piece of feedback can help make this project **more usable, more inclusive, and more human**.

**Accessibility is not a feature.  
Itâ€™s a responsibility.**

ğŸš€ Enjoy hands-free computing!
